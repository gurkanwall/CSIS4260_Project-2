{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ffc9edaf-be62-46cf-b6a2-b7df09af5d94",
   "metadata": {},
   "source": [
    "## BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06f72a13-a702-4d6c-aae0-419d356de224",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping completed: 45 articles saved in 20.34 seconds from TechCrunch.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import time\n",
    "\n",
    "# TechCrunch Tech News URL\n",
    "TECHCRUNCH_URL = \"https://techcrunch.com/\"\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "}\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Request the TechCrunch homepage\n",
    "response = requests.get(TECHCRUNCH_URL, headers=HEADERS)\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "# Find article links\n",
    "articles = []\n",
    "for link in soup.find_all(\"a\", href=True):\n",
    "    url = link[\"href\"]\n",
    "    if url.startswith(\"https://techcrunch.com/\") and \"/20\" in url and url not in articles:\n",
    "        articles.append(url)\n",
    "\n",
    "# Scrape article details\n",
    "data = []\n",
    "for article_url in articles[:500]:  # Scraping 100 articles\n",
    "    try:\n",
    "        article_response = requests.get(article_url, headers=HEADERS)\n",
    "        article_soup = BeautifulSoup(article_response.text, \"html.parser\")\n",
    "\n",
    "        title = article_soup.find(\"h1\").get_text(strip=True) if article_soup.find(\"h1\") else \"No title\"\n",
    "        content = \" \".join([p.get_text(strip=True) for p in article_soup.find_all(\"p\")])\n",
    "\n",
    "        data.append({\"title\": title, \"url\": article_url, \"content\": content})\n",
    "    except Exception as e:\n",
    "        print(f\"Skipping {article_url}: {e}\")\n",
    "        continue\n",
    "\n",
    "# Save to CSV\n",
    "with open(\"techcrunch_articles.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=[\"title\", \"url\", \"content\"])\n",
    "    writer.writeheader()\n",
    "    writer.writerows(data)\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Scraping completed: {len(data)} articles saved in {end_time - start_time:.2f} seconds from TechCrunch.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c9fde1-b874-4172-8bc1-4431718cca27",
   "metadata": {},
   "source": [
    "## MechanicalSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14ada8e3-48be-4542-ba9c-297e4199d929",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping completed using MechanicalSoup: 45 articles saved in 7.55 seconds.\n"
     ]
    }
   ],
   "source": [
    "import mechanicalsoup\n",
    "import time\n",
    "import csv\n",
    "\n",
    "# Initialize the browser\n",
    "browser = mechanicalsoup.StatefulBrowser(user_agent=\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\")\n",
    "\n",
    "# TechCrunch Tech News URL\n",
    "TECHCRUNCH_URL = \"https://techcrunch.com/\"\n",
    "\n",
    "# Start timing\n",
    "start_time = time.time()\n",
    "\n",
    "# Request the TechCrunch homepage\n",
    "browser.open(TECHCRUNCH_URL)\n",
    "\n",
    "# Find article links\n",
    "article_links = browser.links(url_regex='/20')  # Only match articles from the current year\n",
    "article_links = list(set([link.get('href') for link in article_links]))\n",
    "\n",
    "# Scrape article details\n",
    "data = []\n",
    "for article_url in article_links[:500]:  # Limit to 500 articles\n",
    "    try:\n",
    "        # Open the article page\n",
    "        browser.open(article_url)\n",
    "        \n",
    "        # Extract the article title and content\n",
    "        title = browser.page.find('h1').get_text(strip=True) if browser.page.find('h1') else 'No title'\n",
    "        content = ' '.join([p.get_text(strip=True) for p in browser.page.find_all('p')])\n",
    "        \n",
    "        # Save the data\n",
    "        data.append({\n",
    "            'title': title,\n",
    "            'url': article_url,\n",
    "            'content': content\n",
    "        })\n",
    "    except Exception as e:\n",
    "        print(f\"Skipping {article_url}: {e}\")\n",
    "        continue\n",
    "\n",
    "# Save to CSV\n",
    "with open(\"techcrunch_articles_mechanicalsoup.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=[\"title\", \"url\", \"content\"])\n",
    "    writer.writeheader()\n",
    "    writer.writerows(data)\n",
    "\n",
    "# End timing\n",
    "end_time = time.time()\n",
    "\n",
    "# Print completion message\n",
    "print(f\"Scraping completed using MechanicalSoup: {len(data)} articles saved in {end_time - start_time:.2f} seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb042c5-d713-413d-9cd4-72f540c1ef12",
   "metadata": {},
   "source": [
    "| Scraping Method      | Requested Articles | Articles Scraped | Time Taken (Seconds) | Advantages                                                                                   | Limitations                                                            | Notes                                 |\n",
    "|----------------------|--------------------|------------------|----------------------|---------------------------------------------------------------------------------------------|------------------------------------------------------------------------|---------------------------------------|\n",
    "| **MechanicalSoup**    | 500                | 44               | 2.26                 | - Simpler setup and easy to use.<br>- Lightweight and fast for small-scale scraping.<br>- Works well for simple HTML scraping.<br>- Easy to integrate with other Python libraries. | - Scrapes fewer articles (only 44) despite the request for 500.<br>- Slower in scraping large datasets.<br>- Doesn't handle JavaScript-heavy sites.<br>- Lacks advanced features like concurrency.<br>- Limited error handling. | Limited to 44 articles despite the request for 500. |\n",
    "| **Scrapy**            | 500                | 500              | 4.76                 | - Robust and fast for large-scale scraping.<br>- Handles multiple requests concurrently.<br>- Can scrape JavaScript-heavy sites with additional setup.<br>- Built-in support for pagination and follow-up requests.<br>- Handles data extraction and storage efficiently.<br>- Advanced error handling and retry mechanisms. | - More complex setup.<br>- Requires more resources and memory.<br>- Might need extra configurations for certain tasks.<br>- Steeper learning curve.<br>- Slower to set up for small scraping tasks.<br>- Higher resource consumption for small-scale projects. | Scraped 500 articles as requested.   |\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
